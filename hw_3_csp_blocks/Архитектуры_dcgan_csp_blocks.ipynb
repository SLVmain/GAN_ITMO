{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qWdn2i594E0"
      },
      "source": [
        "\n",
        "# Архитектуры DCGAN с CSP блоками\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WtsXyLdQAJN"
      },
      "source": [
        "Adding CSP blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qh2zA--QdM7W"
      },
      "outputs": [],
      "source": [
        "#CSP batchnorm\n",
        "class Csp_Block(nn.Module):\n",
        "    def __init__(self, ch, ch_out):\n",
        "        super(Csp_Block, self).__init__()\n",
        "\n",
        "        half_f = int(ch_out * 0.5)\n",
        "\n",
        "        self.conv_1 = nn.Sequential(\n",
        "          nn.Conv2d(ch, ch_out, kernel_size = 1, padding='same'),\n",
        "          nn.BatchNorm2d(ch_out),\n",
        "          nn.ReLU(),\n",
        "        ) \n",
        "        \n",
        "        self.conv_2 = nn.Sequential(\n",
        "          nn.Conv2d(ch_out, half_f, kernel_size = 3, padding='same'),\n",
        "          nn.BatchNorm2d(half_f),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(half_f, half_f, kernel_size = 1, padding='same'),\n",
        "          nn.BatchNorm2d(half_f),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(half_f, half_f, kernel_size = 3, padding='same'),\n",
        "          nn.BatchNorm2d(half_f),\n",
        "          nn.ReLU(),\n",
        "        ) \n",
        "\n",
        "        self.conv_2_1 = nn.Sequential(\n",
        "          nn.Conv2d(ch_out, half_f, kernel_size = 1, padding='same'),\n",
        "        ) \n",
        "\n",
        "        self.conv_3 = nn.Sequential(\n",
        "          nn.Conv2d(half_f, half_f, kernel_size = 1, padding='same'),\n",
        "          nn.BatchNorm2d(half_f),\n",
        "          nn.ReLU(),\n",
        "        )\n",
        "        #self.relu = nn.ReLU()\n",
        "      \n",
        "    def forward(self, x):\n",
        "        \n",
        "          x = self.conv_1(x)\n",
        "          csp = self.conv_2_1(x)\n",
        "          x = self.conv_2(x)\n",
        "          \n",
        "          x = x + csp\n",
        "          out = self.conv_3(x)\n",
        "          #out = self.relu(x)\n",
        "          return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #CSP without batchnorm\n",
        "class Csp_Block(nn.Module):\n",
        "    def __init__(self, ch, ch_out):\n",
        "        super(Csp_Block, self).__init__()\n",
        "\n",
        "        half_f = int(ch_out * 0.5)\n",
        "\n",
        "        self.conv_1 = nn.Sequential(\n",
        "          nn.Conv2d(ch, ch_out, kernel_size = 1, padding='same'),\n",
        "          nn.ReLU(),\n",
        "        ) \n",
        "        \n",
        "        self.conv_2 = nn.Sequential(\n",
        "          nn.Conv2d(ch_out, half_f, kernel_size = 3, padding='same'),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(half_f, half_f, kernel_size = 1, padding='same'),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(half_f, half_f, kernel_size = 3, padding='same'),\n",
        "          nn.ReLU(),\n",
        "        ) \n",
        "\n",
        "        self.conv_2_1 = nn.Sequential(\n",
        "          nn.Conv2d(ch_out, half_f, kernel_size = 1, padding='same'),\n",
        "        ) \n",
        "\n",
        "        self.conv_3 = nn.Sequential(\n",
        "          nn.Conv2d(half_f, half_f, kernel_size = 1, padding='same'),\n",
        "          nn.ReLU(),\n",
        "        )\n",
        "        #self.relu = nn.ReLU()\n",
        "      \n",
        "    def forward(self, x):\n",
        "        \n",
        "          x = self.conv_1(x)\n",
        "          csp = self.conv_2_1(x)\n",
        "          x = self.conv_2(x)\n",
        "          \n",
        "          x = x + csp\n",
        "          out = self.conv_3(x)\n",
        "          #out = self.relu(x)\n",
        "          return out"
      ],
      "metadata": {
        "id": "rtsfZ4IFMPIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "_ZLnGrr429Pb"
      },
      "outputs": [],
      "source": [
        "#CSP UP with Batchnorm \n",
        "\n",
        "class Csp_Block_Up(nn.Module):\n",
        "    def __init__(self, ch, ch_out):\n",
        "        super(Csp_Block_Up, self).__init__()\n",
        "\n",
        "        half_f = int(ch_out * 0.5)\n",
        "\n",
        "        self.conv_1 = nn.Sequential(\n",
        "          nn.Conv2d(ch, ch_out, kernel_size = 4, padding='same'),\n",
        "          nn.BatchNorm2d(ch_out),\n",
        "          nn.ReLU(),\n",
        "        ) \n",
        "        \n",
        "        self.conv_2 = nn.Sequential(\n",
        "          nn.Conv2d(ch_out, half_f, kernel_size = 1, padding='same'),\n",
        "          nn.BatchNorm2d(half_f),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(half_f, half_f, 4, 2, 1, bias=False),\n",
        "        ) \n",
        "\n",
        "        self.conv_2_1 = nn.Sequential(\n",
        "          nn.Conv2d(half_f, half_f, kernel_size = 8, padding='same'),\n",
        "          nn.BatchNorm2d(half_f),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(half_f, half_f, kernel_size = 8, padding='same'),\n",
        "          nn.BatchNorm2d(half_f),\n",
        "          nn.ReLU(),\n",
        "        ) \n",
        "\n",
        "        self.conv_3_1 = nn.Sequential(\n",
        "          nn.ConvTranspose2d(ch_out, half_f, 4, 2, 1, bias=False),\n",
        "        ) \n",
        "\n",
        "        self.conv_4 = nn.Sequential(\n",
        "          nn.Conv2d(half_f, half_f, kernel_size = 1, padding='same'),\n",
        "          nn.BatchNorm2d(half_f),\n",
        "          nn.ReLU(),\n",
        "        )\n",
        "        #self.relu = nn.ReLU()\n",
        "      \n",
        "    def forward(self, x):\n",
        "        \n",
        "          x = self.conv_1(x)\n",
        "          csp = self.conv_3_1(x)\n",
        "          x = self.conv_2(x)\n",
        "          x = self.conv_2_1(x)\n",
        "          \n",
        "          x = x + csp\n",
        "          out = self.conv_4(x)\n",
        "          #out = self.relu(x)\n",
        "          return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CSP UP without Batchnorm \n",
        "\n",
        "class Csp_Block_Up(nn.Module):\n",
        "    def __init__(self, ch, ch_out):\n",
        "        super(Csp_Block_Up, self).__init__()\n",
        "\n",
        "        half_f = int(ch_out * 0.5)\n",
        "\n",
        "        self.conv_1 = nn.Sequential(\n",
        "          nn.Conv2d(ch, ch_out, kernel_size = 4, padding='same'),\n",
        "          nn.ReLU(),\n",
        "        ) \n",
        "        \n",
        "        self.conv_2 = nn.Sequential(\n",
        "          nn.Conv2d(ch_out, half_f, kernel_size = 1, padding='same'),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(half_f, half_f, 4, 2, 1, bias=False),\n",
        "        ) \n",
        "\n",
        "        self.conv_2_1 = nn.Sequential(\n",
        "          nn.Conv2d(half_f, half_f, kernel_size = 8, padding='same'),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(half_f, half_f, kernel_size = 8, padding='same'),\n",
        "          nn.ReLU(),\n",
        "        ) \n",
        "\n",
        "        self.conv_3_1 = nn.Sequential(\n",
        "          nn.ConvTranspose2d(ch_out, half_f, 4, 2, 1, bias=False),\n",
        "        ) \n",
        "\n",
        "        self.conv_4 = nn.Sequential(\n",
        "          nn.Conv2d(half_f, half_f, kernel_size = 1, padding='same'),\n",
        "          nn.ReLU(),\n",
        "        )\n",
        "      \n",
        "    def forward(self, x):\n",
        "        \n",
        "          x = self.conv_1(x)\n",
        "          csp = self.conv_3_1(x)\n",
        "          x = self.conv_2(x)\n",
        "          x = self.conv_2_1(x)\n",
        "          \n",
        "          x = x + csp\n",
        "          out = self.conv_4(x)\n",
        "          \n",
        "          return out"
      ],
      "metadata": {
        "id": "ijAaNBNfKU27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "QEBovUBo94E_"
      },
      "outputs": [],
      "source": [
        "# Generator Code with CSP (one block)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            \n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            Csp_Block_Up(ngf * 2, ngf * 2), #CSP11111\n",
        "\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator Code without CSP\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            \n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            \n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "metadata": {
        "id": "9nEqdzesK2Zr"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "A4PcdzZs94FB"
      },
      "outputs": [],
      "source": [
        "# Discriminator CSP (4 blocks)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            Csp_Block(ndf, ndf * 2),\n",
        "\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            Csp_Block(ndf * 2, ndf * 2 * 2),\n",
        "\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            Csp_Block(ndf * 4, ndf * 4 * 2),\n",
        "\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            Csp_Block(ndf * 8, ndf * 8 * 2),\n",
        "\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator without CSP (4 blocks)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "metadata": {
        "id": "C_IhmhhBK-kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGfkhrTj94FE"
      },
      "source": [
        "## Выводы:\n",
        "\n",
        "Попробованы три архитектуры модели: \n",
        "1 - DCGAN\n",
        "результат: визуально лучший по качеству фейковых фото и графику лосса\n",
        "clearml ссылка \n",
        "https://app.clear.ml/projects/1e1a4bbf56744c26b6eafa267510eeee/experiments/6c0b57ee203240769987fde34aaec815/output/execution\n",
        "\n",
        "2 - DCGAN c CSP блоками в дискриминаторе с батч-нормализацией и без нормализации\n",
        "с батч нормализацией получен лучше результат, график лосса неплохой, но нужно еще дообучать несколько эпох, выглядит перспективно, но не хватило возможностей GPU доучить дальше\n",
        "clearml ссылка\n",
        "https://app.clear.ml/projects/1e1a4bbf56744c26b6eafa267510eeee/experiments/2f78d059c9874bcdbbf158d36fd70850/output/execution\n",
        "\n",
        "3 - DCGAN c CSP блоками в дискриминаторе и генераторе - не сошелся, генератор не смог научиться\n",
        "clearml ссылка\n",
        "https://app.clear.ml/projects/1e1a4bbf56744c26b6eafa267510eeee/experiments/7db0cbe5d3eb496e90bb6ad0f01a8502/output/execution\n",
        "\n",
        "в папке с домашним заданием три файла фейковых фото: 1, 2, 3 соответственно\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}